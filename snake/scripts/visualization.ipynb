{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea547e20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08e7f5-9674-4f7f-8f4d-ef05b45cb10b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from bs_gym.gymbattlesnake import BattlesnakeEnv\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "\n",
    "import sys\n",
    "\n",
    "from policy import SnakePolicyBase, create_policy\n",
    "from utils import n_opponents, device\n",
    "from utils import PathHelper, plot_graphs\n",
    "from performance import check_performance # TODO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a297b1f-a5a6-4590-8d7b-c1728b6d1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u = PathHelper()\n",
    "rollouts = None\n",
    "policy = None\n",
    "policies = None\n",
    "\n",
    "# TODO:\n",
    "n_envs = 1\n",
    "n_steps = 600\n",
    "CPU_THREADS = 6\n",
    "\n",
    "\n",
    "def setup_rollouts() -> BattlesnakeEnv:\n",
    "    global rollouts\n",
    "    tmp_env = BattlesnakeEnv(n_threads=2, n_envs=n_envs)\n",
    "\n",
    "    rollouts = RolloutStorage(n_steps,\n",
    "                            n_envs,\n",
    "                            tmp_env.observation_space.shape,\n",
    "                            tmp_env.action_space,\n",
    "                            n_steps)\n",
    "    tmp_env.close()\n",
    "\n",
    "    return tmp_env\n",
    "\n",
    "# TODO: refactor to utils\n",
    "def setup_policy(tmp_env, model_path=None):\n",
    "    global device\n",
    "\n",
    "    policy = create_policy(tmp_env.observation_space.shape, tmp_env.action_space, SnakePolicyBase)\n",
    "    if model_path is not None:\n",
    "        policy.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    policy.to(device)\n",
    "    policy.eval()\n",
    "\n",
    "    # TODO: load multiple models, load state dict into policies\n",
    "    return policy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8002e-4bdc-4591-8888-09c4676c20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def obs_to_frame(obs):\n",
    "    ''' Converts an environment observation into a renderable RGB image '''\n",
    "    # First, let's find the game board dimensions from layer 5\n",
    "    x_offset, y_offset = 0, 0\n",
    "    done = False\n",
    "    for x in range(23):\n",
    "        if done:\n",
    "            break\n",
    "        for y in range(23):\n",
    "            if obs[0][5][x][y] == 1:\n",
    "                x_offset = x\n",
    "                y_offset = y\n",
    "                done = True\n",
    "                break\n",
    "    output = np.zeros((11, 11, 3), dtype=np.uint8)\n",
    "\n",
    "    # See https://github.com/cbinners/gym-battlesnake/blob/master/gym_battlesnake/src/gamewrapper.cpp#L55 for\n",
    "    # layer reference\n",
    "    # TODO: to improve?\n",
    "    for x in range(23):\n",
    "        for y in range(23):\n",
    "            # Render snake bodies\n",
    "            if obs[0][1][x][y] == 1:\n",
    "                output[x-x_offset][y-y_offset] = 255 - 10*(255 - obs[0][2][x][y])\n",
    "            # Render food\n",
    "            if obs[0][4][x][y] == 1:\n",
    "                output[x-x_offset][y-y_offset][0] = 255\n",
    "                output[x-x_offset][y-y_offset][1] = 255\n",
    "                output[x-x_offset][y-y_offset][2] = 0\n",
    "            # Render snake heads as a red pixel\n",
    "            if obs[0][0][x][y] > 0:\n",
    "                output[x-x_offset][y-y_offset][0] = 255\n",
    "                output[x-x_offset][y-y_offset][1] = 0\n",
    "                output[x-x_offset][y-y_offset][2] = 0\n",
    "            # Render snake heads\n",
    "            if obs[0][6][x][y] == 1:\n",
    "                output[x-x_offset][y-y_offset][0] = 0\n",
    "                output[x-x_offset][y-y_offset][1] = 255\n",
    "                output[x-x_offset][y-y_offset][2] = 0\n",
    "\n",
    "    return output\n",
    "\n",
    "def visualize_game(policy):\n",
    "    playground = BattlesnakeEnv(n_threads=CPU_THREADS, n_envs=1, fixed_orientation=True,\n",
    "                                opponents=policies, device=device,\n",
    "                                )\n",
    "\n",
    "    # Reset the environment \n",
    "    obs = playground.reset()\n",
    "\n",
    "    # Keep track of game frames to render\n",
    "    video = []\n",
    "\n",
    "    # Grab a set of frames to render\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(300)):\n",
    "            # Add the rendered observation to our frame stack\n",
    "            video.append(obs_to_frame(obs))\n",
    "\n",
    "            # Get the action our policy should take\n",
    "            _, action, _, _ = policy.act(torch.tensor(obs, dtype=torch.float32).to(device), None, None)\n",
    "\n",
    "            # Perform our action and update our observation\n",
    "            obs,_,_,_ = playground.step(action.cpu().squeeze())\n",
    "\n",
    "    # Render, adapted from here: https://stackoverflow.com/questions/57060422/fast-way-to-display-video-from-arrays-in-jupyter-lab\n",
    "\n",
    "    video = np.array(video, dtype=np.uint8)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    im = plt.imshow(video[0,:,:,:])\n",
    "    def init():\n",
    "        im.set_data(video[0,:,:,:])\n",
    "    def animate(i):\n",
    "        im.set_data(video[i,:,:,:])\n",
    "        return im\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0], interval=200)\n",
    "    return anim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5686ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "u.set_modelgroup('test10', read_tmp=True)\n",
    "model_path = None\n",
    "\n",
    "latest_model_path, cur_iter = u.get_latest_model()\n",
    "print(latest_model_path)\n",
    "if latest_model_path is not None:\n",
    "    model_path = latest_model_path\n",
    "else:\n",
    "    sys.exit(1)\n",
    "\n",
    "print('Loading model from:', model_path)\n",
    "\n",
    "# print(torch.load(model_path))\n",
    "\n",
    "tmp_env = setup_rollouts()\n",
    "policy = setup_policy(tmp_env, model_path=model_path)\n",
    "if policy is None:\n",
    "    print('Policy is None')\n",
    "    sys.exit(1)\n",
    "\n",
    "policies = [policy for _ in range(n_opponents)]\n",
    "\n",
    "data = u.load_data(start_iteration=cur_iter)\n",
    "rewards = data['rewards']\n",
    "value_losses = data['value_losses']\n",
    "action_losses = data['action_losses']\n",
    "dist_entropies = data['dist_entropies']\n",
    "lengths = data['lengths']\n",
    "\n",
    "# print(data)\n",
    "\n",
    "plot_graphs(rewards, value_losses, action_losses, dist_entropies, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e70a6-d1a4-41b3-8dfb-b0008cddfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anim = visualize_game(policy)\n",
    "# HTML(anim.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: automate finding next best model(s)\n",
    "models_to_test = [\n",
    "    # 'models/test10/tmp_iter1400.pt',\n",
    "    'models/test10/tmp_iter1200.pt',\n",
    "    'models/test10/tmp_iter1000.pt',\n",
    "    'models/test10/tmp_iter800.pt',\n",
    "    'models/test10/tmp_iter600.pt',\n",
    "]\n",
    "\n",
    "def regression_test():\n",
    "    for next_best_modelpath in models_to_test:\n",
    "        next_best_policy = setup_policy(tmp_env, model_path=next_best_modelpath)\n",
    "        winrate = check_performance(policy, next_best_policy, device=torch.device(device))\n",
    "        \n",
    "        print(f'Winrate: {winrate}')\n",
    "\n",
    "# regression_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97d491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
