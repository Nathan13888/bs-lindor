{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd ../bs-gym/ && pip install -e . -q\n",
    "# !cd .\n",
    "# !cd ../pytorch-a2c-ppo-acktr-gail/ && pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which python\n",
    "# !which pip\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from a2c_ppo_acktr.algo import PPO\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from bs_gym.gymbattlesnake import BattlesnakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from performance import check_performance\n",
    "from policy import SnakePolicyBase, create_policy\n",
    "\n",
    "from utils import n_opponents,  device\n",
    "from utils import PathHelper, plot_graphs\n",
    "\n",
    "u = PathHelper()\n",
    "\n",
    "rollouts = None\n",
    "tmp_env = None\n",
    "n_envs = -1\n",
    "n_steps = -1\n",
    "CPU_THREADS = -1\n",
    "\n",
    "def setup_rollouts(n_envs) -> None:\n",
    "    global rollouts, tmp_env\n",
    "\n",
    "    print('Setting up rollouts...')\n",
    "    tmp_env = BattlesnakeEnv(n_threads=2, n_envs=n_envs)\n",
    "\n",
    "    # rollout storage: game turns played and rewards\n",
    "    rollouts = RolloutStorage(n_steps,\n",
    "                            n_envs,\n",
    "                            tmp_env.observation_space.shape,\n",
    "                            tmp_env.action_space,\n",
    "                            n_steps)\n",
    "    tmp_env.close()\n",
    "\n",
    "policy = None\n",
    "best_old_policy = None\n",
    "agent = None\n",
    "\n",
    "def setup_agent(model_path=None,\n",
    "                value_loss_coef=0.5,\n",
    "                entropy_coef=0.01,\n",
    "                max_grad_norm=0.5,\n",
    "                clip_param=0.2,\n",
    "                ppo_epoch=4,\n",
    "                num_mini_batch=16,\n",
    "                eps=1e-5,\n",
    "                lr=5e-5,\n",
    "                use_clipped_value_loss=True,\n",
    "                use_gradient_accumulation=False,\n",
    "                gradient_accumulation_steps=1\n",
    "                ) -> None:\n",
    "    global policy, best_old_policy, agent\n",
    "    print('Setting up agent...')\n",
    "\n",
    "    # policies\n",
    "    policy = create_policy(tmp_env.observation_space.shape, tmp_env.action_space, SnakePolicyBase)\n",
    "    # load latest model if found\n",
    "    if model_path is not None:\n",
    "        policy.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    best_old_policy = create_policy(tmp_env.observation_space.shape, tmp_env.action_space, SnakePolicyBase)\n",
    "    best_old_policy.load_state_dict(policy.state_dict())\n",
    "\n",
    "    # TODO: second_best model\n",
    "\n",
    "    # agent\n",
    "    agent = PPO(policy,\n",
    "                value_loss_coef=value_loss_coef,\n",
    "                entropy_coef=entropy_coef,\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                clip_param=clip_param,\n",
    "                ppo_epoch=ppo_epoch,\n",
    "                num_mini_batch=num_mini_batch,\n",
    "                eps=eps,\n",
    "                lr=lr,\n",
    "                use_clipped_value_loss=use_clipped_value_loss,\n",
    "                use_gradient_accumulation=use_gradient_accumulation,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                )\n",
    "\n",
    "env = None\n",
    "obs = None\n",
    "\n",
    "def setup_env() -> None:\n",
    "    global env, obs\n",
    "    print('Setting up environment...')\n",
    "    \n",
    "    # sanity check\n",
    "    assert policy is not None\n",
    "    assert best_old_policy is not None\n",
    "    assert agent is not None\n",
    "\n",
    "    # environment\n",
    "    env = BattlesnakeEnv(n_threads=CPU_THREADS, n_envs=n_envs, opponents=[policy for _ in range(n_opponents)], device=device)\n",
    "    # TODO: second_best model\n",
    "\n",
    "    obs = env.reset()\n",
    "    rollouts.obs[0].copy_(torch.tensor(obs))\n",
    "\n",
    "    # send network/storage to gpu\n",
    "    policy.to(device)\n",
    "    best_old_policy.to(device)\n",
    "    rollouts.to(device)\n",
    "\n",
    "def count_parameters(model) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_iteration = -1\n",
    "\n",
    "rewards = []\n",
    "value_losses = []\n",
    "action_losses = []\n",
    "dist_entropies = []\n",
    "lengths = []\n",
    "\n",
    "def train(num_updates, start_iteration=0, test_every=5, required_winrate=0.3, force_save=-1,\n",
    "          gamma=0.99, lambd=0.95, datafile=None):\n",
    "    print(\"Starting training.\")\n",
    "    global last_iteration, rewards, value_losses, lengths\n",
    "\n",
    "    data = u.load_data(datafile=datafile, start_iteration=start_iteration)\n",
    "    rewards = data['rewards']\n",
    "    value_losses = data['value_losses']\n",
    "    action_losses = data['action_losses']\n",
    "    dist_entropies = data['dist_entropies']\n",
    "    lengths = data['lengths']\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(num_updates):\n",
    "        j = start_iteration + i\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        \n",
    "        policy.eval()\n",
    "        print(f\"Iteration {j}: Generating rollouts\")\n",
    "        for step in tqdm(range(n_steps)):\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob, recurrent_hidden_states = policy.act(rollouts.obs[step],\n",
    "                                                                rollouts.recurrent_hidden_states[step],\n",
    "                                                                rollouts.masks[step])\n",
    "            obs, reward, done, infos = env.step(action.cpu().squeeze())\n",
    "            obs = torch.tensor(obs)\n",
    "            reward = torch.tensor(reward).unsqueeze(1)\n",
    "\n",
    "            for info in infos:\n",
    "                if 'episode' in info.keys():\n",
    "                    episode_rewards.append(info['episode']['r'])\n",
    "                    episode_lengths.append(info['episode']['l'])\n",
    "\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            bad_masks = torch.FloatTensor([[0.0] if 'bad_transition' in info.keys() else [1.0] for info in infos])\n",
    "            rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = policy.get_value(\n",
    "                rollouts.obs[-1],\n",
    "                rollouts.recurrent_hidden_states[-1],\n",
    "                rollouts.masks[-1]\n",
    "            ).detach()\n",
    "            \n",
    "        policy.train()\n",
    "\n",
    "        print(\"Training policy on rollouts...\")\n",
    "        rollouts.compute_returns(next_value, True, gamma, lambd, False)\n",
    "        value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "        rollouts.after_update()\n",
    "\n",
    "        policy.eval()\n",
    "        \n",
    "        total_num_steps = (j + 1) * n_envs * n_steps\n",
    "        end = time.time()\n",
    "        \n",
    "        lengths.append(np.mean(episode_lengths))\n",
    "        rewards.append(np.mean(episode_rewards))\n",
    "        value_losses.append(value_loss)\n",
    "        action_losses.append(action_loss)\n",
    "        dist_entropies.append(dist_entropy)\n",
    "        \n",
    "        print(f\"Time taken: {end - start:.2f} seconds\")\n",
    "        print(f\"Completed {total_num_steps} steps\")\n",
    "\n",
    "        u.save_data(rewards=rewards,\n",
    "                    value_losses= value_losses,\n",
    "                    action_losses=action_losses,\n",
    "                    dist_entropies=dist_entropies, \n",
    "                    lengths=lengths, \n",
    "                    iteration=j, datafile=datafile)\n",
    "\n",
    "        saved_model = False\n",
    "        if j % test_every == 0 and j != 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"=\" * 80)\n",
    "            print(\"Iteration\", j, \"Results\")\n",
    "            # TODO: do in parallel\n",
    "            # Check the performance of the current policy against the prior best\n",
    "            winrate = check_performance(policy, best_old_policy, device=torch.device(device))#device=device)\n",
    "            print(f\"Winrate vs prior best: {winrate*100:.2f}%\")\n",
    "            print(f\"Median Length: {np.median(episode_lengths)}\")\n",
    "            print(f\"Max Length: {np.max(episode_lengths)}\")\n",
    "            print(f\"Min Length: {np.min(episode_lengths)}\")\n",
    "\n",
    "            if winrate > required_winrate:\n",
    "                print(\"Policy winrate is > 30%. Updating prior best model\")\n",
    "                best_old_policy.load_state_dict(policy.state_dict())\n",
    "                print(\"Saving latest best model.\")\n",
    "                saved_model = True\n",
    "                torch.save(best_old_policy.state_dict(), u.get_modelpath(iteration=j))\n",
    "            else:\n",
    "                print(\"Policy has not learned enough yet... keep training!\")\n",
    "            print(\"-\" * 80)\n",
    "        if not saved_model and (force_save > 0 and j % force_save == 0):\n",
    "            print(\"Force saving latest model.\")\n",
    "            torch.save(policy.state_dict(), u.get_modelpath(custom=f'tmp_iter', iteration=j))\n",
    "            \n",
    "    print(\"Saving final model.\")\n",
    "    last_iteration = start_iteration + num_updates - 1\n",
    "    torch.save(policy.state_dict(), u.get_modelpath(last_iteration))\n",
    "    torch.save(policy.state_dict(), u.get_modelpath_latest())\n",
    "\n",
    "    return rewards, value_losses, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do matmul at TF32 mode.\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "\n",
    "# FORMULA: The total training set size per iteration is n_envs * n_steps\n",
    "n_envs = 400     # parallel environments\n",
    "n_steps = 600    # steps per environment to simulate\n",
    "CPU_THREADS = 6\n",
    "\n",
    "num_updates = 100 # iterations to run\n",
    "test_every = 10 # iterations per test\n",
    "required_winrate = 0.3\n",
    "force_save = 5 # iterations per force save\n",
    "# TODO: flexible test_every\n",
    "\n",
    "MODEL_GROUP = 'test7'\n",
    "\n",
    "u.set_modelgroup(MODEL_GROUP, read_tmp=True)\n",
    "\n",
    "\n",
    "\n",
    "# =====================================\n",
    "latest_model_path, iteration = u.get_latest_model()\n",
    "print(f\"Latest model: {latest_model_path}\")\n",
    "print(f\"Latest iteration: {iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "setup_rollouts(n_envs)\n",
    "\n",
    "setup_agent(model_path=latest_model_path,\n",
    "            value_loss_coef=0.5,\n",
    "            entropy_coef=0.01,\n",
    "            max_grad_norm=0.5,\n",
    "            clip_param=0.2,\n",
    "            num_mini_batch=64,\n",
    "            ppo_epoch=4,\n",
    "            eps=1e-5,\n",
    "            # lr=3e-4,\n",
    "            lr=5e-5,\n",
    "            use_clipped_value_loss=True,\n",
    "            use_gradient_accumulation=False,\n",
    "            gradient_accumulation_steps=1\n",
    "            )\n",
    "print(f\"Trainable Parameters: {count_parameters(policy)}\")\n",
    "\n",
    "\n",
    "setup_env()\n",
    "\n",
    "rewards2, value_losses2, lengths2 = train(num_updates, start_iteration=iteration+1,\n",
    "                                          test_every=test_every,\n",
    "                                          required_winrate=required_winrate,\n",
    "                                          force_save=force_save,\n",
    "                                          gamma=0.99,\n",
    "                                          lambd=0.95,\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "# TODO: send notification when complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(rewards, value_losses, action_losses, dist_entropies, lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
