{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../bs-gym/ && pip install -e . -q\n",
    "!cd .\n",
    "!cd ../pytorch-a2c-ppo-acktr-gail/ && pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python\n",
    "!which pip\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from a2c_ppo_acktr.algo import PPO\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from bs_gym.gymbattlesnake import BattlesnakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Creating environment.\")\n",
    "\n",
    "from performance import check_performance\n",
    "from policy import SnakePolicyBase, create_policy\n",
    "\n",
    "from utils import n_envs, n_steps, n_opponents, CPU_THREADS, device\n",
    "from utils import PathHelper\n",
    "\n",
    "u = PathHelper()\n",
    "\n",
    "tmp_env = BattlesnakeEnv(n_threads=CPU_THREADS, n_envs=n_envs)\n",
    "\n",
    "# rollout storage: game turns played and rewards\n",
    "rollouts = RolloutStorage(n_steps,\n",
    "                          n_envs,\n",
    "                          tmp_env.observation_space.shape,\n",
    "                          tmp_env.action_space,\n",
    "                          n_steps)\n",
    "tmp_env.close()\n",
    "\n",
    "policy = None\n",
    "best_old_policy = None\n",
    "agent = None\n",
    "\n",
    "def setup_agent(model_path=None):\n",
    "    global policy, best_old_policy, agent\n",
    "\n",
    "    # policies\n",
    "    policy = create_policy(tmp_env.observation_space.shape, tmp_env.action_space, SnakePolicyBase)\n",
    "    # load latest model if found\n",
    "    if model_path is not None:\n",
    "        policy.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    best_old_policy = create_policy(tmp_env.observation_space.shape, tmp_env.action_space, SnakePolicyBase)\n",
    "    best_old_policy.load_state_dict(policy.state_dict())\n",
    "\n",
    "    # TODO: second_best model\n",
    "\n",
    "    # agent\n",
    "    # TODO: configure elsewhere\n",
    "    agent = PPO(policy,\n",
    "                value_loss_coef=0.5,\n",
    "                entropy_coef=0.01,\n",
    "                max_grad_norm=0.5,\n",
    "                clip_param=0.2,\n",
    "                ppo_epoch=4,\n",
    "                num_mini_batch=16,\n",
    "                eps=1e-5,\n",
    "                lr=5e-5,\n",
    "                )\n",
    "\n",
    "env = None\n",
    "obs = None\n",
    "\n",
    "def setup_env():\n",
    "    global env, obs\n",
    "    \n",
    "    # sanity check\n",
    "    assert policy is not None\n",
    "    assert best_old_policy is not None\n",
    "    assert agent is not None\n",
    "\n",
    "    # environment\n",
    "    env = BattlesnakeEnv(n_threads=CPU_THREADS, n_envs=n_envs, opponents=[policy for _ in range(3)], device=device)\n",
    "    # TODO: second_best model\n",
    "\n",
    "    obs = env.reset()\n",
    "    rollouts.obs[0].copy_(torch.tensor(obs))\n",
    "\n",
    "    # send network/storage to gpu\n",
    "    policy.to(device)\n",
    "    best_old_policy.to(device)\n",
    "    rollouts.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def plot_graphs(rewards, value_losses, lengths):\n",
    "    plt.clf()\n",
    "\n",
    "    plt.title(\"Average Length vs Episode\")\n",
    "    plt.ylabel(\"Length\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.plot(lengths)\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Average Loss vs Episode\")\n",
    "    plt.ylabel(\"Length\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.plot(lengths)\n",
    "    plt.show(value_losses)\n",
    "\n",
    "    plt.title(\"Average Reward vs Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_iteration = -1\n",
    "\n",
    "rewards = []\n",
    "value_losses = []\n",
    "lengths = []\n",
    "\n",
    "def train(num_updates, start_iteration=0, test_every=5):\n",
    "    print(\"Starting training.\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(num_updates):\n",
    "        j = start_iteration + i\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        \n",
    "        policy.eval()\n",
    "        print(f\"Iteration {j}: Generating rollouts\")\n",
    "        for step in tqdm(range(n_steps)):\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob, recurrent_hidden_states = policy.act(rollouts.obs[step],\n",
    "                                                                rollouts.recurrent_hidden_states[step],\n",
    "                                                                rollouts.masks[step])\n",
    "            obs, reward, done, infos = env.step(action.cpu().squeeze())\n",
    "            obs = torch.tensor(obs)\n",
    "            reward = torch.tensor(reward).unsqueeze(1)\n",
    "\n",
    "            for info in infos:\n",
    "                if 'episode' in info.keys():\n",
    "                    episode_rewards.append(info['episode']['r'])\n",
    "                    episode_lengths.append(info['episode']['l'])\n",
    "\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            bad_masks = torch.FloatTensor([[0.0] if 'bad_transition' in info.keys() else [1.0] for info in infos])\n",
    "            rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = policy.get_value(\n",
    "                rollouts.obs[-1],\n",
    "                rollouts.recurrent_hidden_states[-1],\n",
    "                rollouts.masks[-1]\n",
    "            ).detach()\n",
    "            \n",
    "        policy.train()\n",
    "\n",
    "        print(\"Training policy on rollouts...\")\n",
    "        # We're using a gamma = 0.99 and lambda = 0.95\n",
    "        rollouts.compute_returns(next_value, True, 0.99, 0.95, False)\n",
    "        value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "        rollouts.after_update()\n",
    "\n",
    "        policy.eval()\n",
    "        \n",
    "        total_num_steps = (j + 1) * n_envs * n_steps\n",
    "        end = time.time()\n",
    "        \n",
    "        lengths.append(np.mean(episode_lengths))\n",
    "        rewards.append(np.mean(episode_rewards))\n",
    "        value_losses.append(value_loss)\n",
    "        \n",
    "        # TODO: modify 5 to be a variable\n",
    "        if j % check_every == 0 and j != 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"=\" * 80)\n",
    "            print(\"Iteration\", j, \"Results\")\n",
    "            # TODO: do in parallel\n",
    "            # Check the performance of the current policy against the prior best\n",
    "            winrate = check_performance(policy, best_old_policy, device=torch.device(device))#device=device)\n",
    "            print(f\"Winrate vs prior best: {winrate*100:.2f}%\")\n",
    "            print(f\"Median Length: {np.median(episode_lengths)}\")\n",
    "            print(f\"Max Length: {np.max(episode_lengths)}\")\n",
    "            print(f\"Min Length: {np.min(episode_lengths)}\")\n",
    "\n",
    "            # TODO: modify winrate requirement to be a variable, eg. 30% (for 4 players)\n",
    "            if winrate > 0.3:\n",
    "                print(\"Policy winrate is > 30%. Updating prior best model\")\n",
    "                best_old_policy.load_state_dict(policy.state_dict())\n",
    "                print(\"Saving latest best model.\")\n",
    "                \n",
    "                torch.save(best_old_policy.state_dict(), u.get_modelpath(iteration=j))\n",
    "            else:\n",
    "                print(\"Policy has not learned enough yet... keep training!\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    print(\"Saving final model.\")\n",
    "    last_iteration = start_iteration + num_updates - 1\n",
    "    torch.save(policy.state_dict(), u.get_modelpath_latest())\n",
    "\n",
    "    return rewards, value_losses, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do matmul at TF32 mode.\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "\n",
    "u.set_modelgroup('test')\n",
    "\n",
    "num_updates = 5 # iterations to run\n",
    "# num_updates = 4200 # for competition training\n",
    "\n",
    "latest_model_path, iteration = u.get_latest_model()\n",
    "print(f\"Latest model: {latest_model_path}\")\n",
    "print(f\"Latest iteration: {iteration}\")\n",
    "\n",
    "setup_agent(model_path=latest_model_path)\n",
    "setup_env()\n",
    "\n",
    "\n",
    "rewards2, value_losses2, lengths2 = train(num_updates, start_iteration=iteration+1\n",
    "                                          test_every=10\n",
    "                                          )\n",
    "# TODO: send notification when complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trainable Parameters: {count_parameters(policy)}\")\n",
    "# TODO: last model\n",
    "# TODO: iterations\n",
    "\n",
    "\n",
    "plot_graphs(rewards, value_losses, lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
